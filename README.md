# DeltaPapers
A Paper List for Parameter-efficient Tuning (Delta Tuning).

## Classification Criteria

## Papers
- Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models
- Parameter-Efficient Transfer Learning for NLP. 
- The Power of Scale for Parameter-Efficient Prompt Tuning
- Towards a Unified View of Parameter-Efficient Transfer Learning
- LoRA: Low-Rank Adaptation of Large Language Models
- Prefix-Tuning: Optimizing Continuous Prompts for Generation
- AdapterHub: A Framework for Adapting Transformers
- AdapterBias: Parameter-efficient Token-dependent Embedding Shift for Adapters in NLP Tasks
- AdapterFusion: Non-Destructive Task Composition for Transfer Learning
- COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers
- On the Effectiveness Adapter-based Tuning for Pretrained Language Model Adaptation
- Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks
- Masking as an Efficient Alternative to Finetuning for Pretrained Language Models
- UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning
- CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA
- BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models
- gpt understands too.
- Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning
- Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning
- INTRINSIC DIMENSIONALITY EXPLAINS THE EFFEC- TIVENESS OF LANGUAGE MODEL FINE-TUNING
- Parameter-Efficient Transfer Learning with Diff Pruning
- Training Neural Networks with Fixed Sparse Masks
- Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators
- Fast Model Editing at Scale
- Editing Factual Knowledge in Language Models
- LiST: Lite Self-training Makes Efficient Few-shot Learners
- Movement Pruning: Adaptive Sparsity by Fine-Tuning
- VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks
- BEYOND FULLY-CONNECTED LAYERS WITH QUATERNIONS: PARAMETERIZATION OF HYPERCOMPLEX MULTIPLICATIONS WITH 1/n PARAMETERS
- Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices
- Adapterdrop: On the efficiency of adapters in transformers.
- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task
- Lightweight Adapter Tuning for Multilingual Speech Translation

