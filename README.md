# DeltaPapers
A Paper List for Parameter-efficient Tuning (Delta Tuning).

## Introduction
This is a paper list of parameter-efficient methods (Delta Tuning) for pre-trained models. 


### Keywords Convention

We follow the general idea of PromptPapers to label the papers. 

![](https://img.shields.io/badge/T5-blue) The abbreviation of the work.

![](https://img.shields.io/badge/Specification-red) The primary class according to the classification criteion.

![](https://img.shields.io/badge/Generation-brown) The main explored task of the work.

![](https://img.shields.io/badge/MultiTask-green) The main explored property of delta tuning methods in the work.

## Papers

### Overview

- **Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Model**, Preprint 2022. 

  *Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, Maosong Sun*. [[pdf](https://arxiv.org/abs/2203.06904)], [[OpenDelta](https://github.com/thunlp/OpenDelta)] 

- **Parameter-Efficient Transfer Learning for NLP,** ICML 2019. 

  Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly. [[pdf](https://arxiv.org/abs/1902.00751)], [[project](https://github.com/google-research/adapter-bert)]

- **The Power of Scale for Parameter-Efficient Prompt Tuning,** EMNLP 2021.

  Brian Lester, Rami Al-Rfou, Noah Constant. [[pdf](https://arxiv.org/abs/2104.08691)]

- **Towards a Unified View of Parameter-Efficient Transfer Learning,** ICLR 2022.

  Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig. [[pdf](https://arxiv.org/abs/2110.04366)], [[project](https://github.com/jxhe/unify-parameter-efficient-tuning)]

- **LoRA: Low-Rank Adaptation of Large Language Models,** ICLR 2022.

  Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. [[pdf](https://arxiv.org/abs/2106.09685)], [[project](https://github.com/microsoft/LoRA)]

- **Prefix-Tuning: Optimizing Continuous Prompts for Generation,** ACL 2021.

  Xiang Lisa Li, Percy Liang. [[pdf](https://arxiv.org/abs/2101.00190)], [[project]()]

- **AdapterHub: A Framework for Adapting Transformers,** 

  Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych. [[pdf](https://arxiv.org/abs/2007.07779)], [[project](https://adapterhub.ml/)]

- AdapterBias: Parameter-efficient Token-dependent Embedding Shift for Adapters in NLP Tasks

- AdapterFusion: Non-Destructive Task Composition for Transfer Learning

- COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers

- On the Effectiveness Adapter-based Tuning for Pretrained Language Model Adaptation

- Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks

- Masking as an Efficient Alternative to Finetuning for Pretrained Language Models

- UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning

- CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA

- BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models

- gpt understands too.

- Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning

- Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning

- INTRINSIC DIMENSIONALITY EXPLAINS THE EFFEC- TIVENESS OF LANGUAGE MODEL FINE-TUNING

- Parameter-Efficient Transfer Learning with Diff Pruning

- Training Neural Networks with Fixed Sparse Masks

- Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators

- Fast Model Editing at Scale

- Editing Factual Knowledge in Language Models

- LiST: Lite Self-training Makes Efficient Few-shot Learners

- Movement Pruning: Adaptive Sparsity by Fine-Tuning

- **VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks,** CVPR 2022.

  *Yi-Lin Sung, Jaemin Cho, Mohit Bansal*. [[pdf](https://arxiv.org/abs/2112.06825)], [[Project](https://github.com/ylsung/VL_adapter)]

- **Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters,** ICLR 2021.

  *Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung Hui, Jie Fu*. [[pdf](https://arxiv.org/abs/2102.08597)]

- **Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices,** Neurips 2021.

  *Aliakbar Panahi, Seyran Saeedi, Tom Arodz*. [[pdf](https://proceedings.neurips.cc/paper/2021/file/09def3ebbc44ff3426b28fcd88c83554-Paper.pdf)], [[Project](https://github.com/tarodz/shapeshifter)]

- **Adapterdrop: On the efficiency of adapters in transformers,** EMNLP 2021.

  *Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, Iryna Gurevych*. [[pdf](https://arxiv.org/abs/2010.11918)]

- **BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task,** ICML 2019.
  *Asa Cooper Stickland, Iain Murray*. [[pdf](https://arxiv.org/abs/1902.02671)], [[Project](https://github.com/AsaCooperStickland/Bert-n-Pals)]


- **Lightweight Adapter Tuning for Multilingual Speech Translation,** ACL 2021.

  *Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier Schwab, Laurent Besacier*. [[pdf](https://arxiv.org/abs/2106.01463)], [[Project](https://github.com/formiel/fairseq/blob/master/examples/speech_to_text/docs/adapters.md)]

