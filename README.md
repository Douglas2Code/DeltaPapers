# DeltaPapers
A Paper List for Parameter-efficient Tuning (Delta Tuning).

## Introduction
This is a paper list of parameter-efficient methods (Delta Tuning) for pre-trained models. 


### Keywords Convention

We follow the general idea of PromptPapers to label the papers. 

![](https://img.shields.io/badge/T5-blue) The abbreviation of the work.

![](https://img.shields.io/badge/Specification-red) The primary class according to the classification criteion.

![](https://img.shields.io/badge/Generation-brown) The main explored task of the work.

![](https://img.shields.io/badge/MultiTask-green) The main explored property of delta tuning methods in the work.

## Papers

### Overview

- **Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Model**, Preprint. ![](https://img.shields.io/badge/Empirical Study-green)![](https://img.shields.io/badge/Theoretical Study-green)

  *Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, Maosong Sun* [[pdf](https://arxiv.org/abs/2203.06904)], [[project](https://github.com/thunlp/OpenDelta)] 

- Parameter-Efficient Transfer Learning for NLP. 

- The Power of Scale for Parameter-Efficient Prompt Tuning

- Towards a Unified View of Parameter-Efficient Transfer Learning

- LoRA: Low-Rank Adaptation of Large Language Models

- Prefix-Tuning: Optimizing Continuous Prompts for Generation

- AdapterHub: A Framework for Adapting Transformers

- AdapterBias: Parameter-efficient Token-dependent Embedding Shift for Adapters in NLP Tasks

- AdapterFusion: Non-Destructive Task Composition for Transfer Learning

- COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers

- On the Effectiveness Adapter-based Tuning for Pretrained Language Model Adaptation

- Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks

- Masking as an Efficient Alternative to Finetuning for Pretrained Language Models

- UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning

- CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA

- BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models

- gpt understands too.

- Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning

- Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning

- INTRINSIC DIMENSIONALITY EXPLAINS THE EFFEC- TIVENESS OF LANGUAGE MODEL FINE-TUNING

- Parameter-Efficient Transfer Learning with Diff Pruning

- Training Neural Networks with Fixed Sparse Masks

- Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators

- Fast Model Editing at Scale

- Editing Factual Knowledge in Language Models

- LiST: Lite Self-training Makes Efficient Few-shot Learners

- Movement Pruning: Adaptive Sparsity by Fine-Tuning

- VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks

- BEYOND FULLY-CONNECTED LAYERS WITH QUATERNIONS: PARAMETERIZATION OF HYPERCOMPLEX MULTIPLICATIONS WITH 1/n PARAMETERS

- Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices

- Adapterdrop: On the efficiency of adapters in transformers.

- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task

- Lightweight Adapter Tuning for Multilingual Speech Translation

